from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import sys
import subprocess

# ----------------------------------------------------------------------------
# 1. Configuration & Paths
# ----------------------------------------------------------------------------
# We define these globally, but we will also ensure they work inside tasks
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # /usr/local/airflow
PROCESSED_DIR = os.path.join(BASE_DIR, "include", "data", "processed")
MLFLOW_ARTIFACTS = os.path.join(BASE_DIR, "include", "mlflow_artifacts")

# Ensure Artifacts Directory Exists
os.makedirs(MLFLOW_ARTIFACTS, exist_ok=True)

default_args = {
    "owner": "astro",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="rps_pipeline",
    default_args=default_args,
    start_date=datetime(2025, 11, 29),
    schedule="@hourly",
    catchup=False,
    tags=["mlops", "rps"],
) as dag:

    # --------------------
    # Task 1: Fetch Data
    # --------------------
    def fetch_data_task():
        # Re-define BASE_DIR inside to be 100% safe against scope issues
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        sys.path.insert(0, os.path.join(base_dir, "src"))
        
        from fetch_data import fetch_weather_data
        return fetch_weather_data()

    fetch_task = PythonOperator(
        task_id="fetch_live_data",
        python_callable=fetch_data_task,
    )

    # --------------------
    # Task 2: Quality Check
    # --------------------
    def quality_check_task(ti):
        raw_file = ti.xcom_pull(task_ids="fetch_live_data")
        
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        sys.path.insert(0, os.path.join(base_dir, "src"))
        
        from quality_check import check_quality
        check_quality(raw_file)

    quality_task = PythonOperator(
        task_id="quality_check",
        python_callable=quality_check_task,
    )

    # --------------------
    # Task 3: Preprocess + Auto-Commit (The Complex One)
    # --------------------
    def preprocess_task(ti):
        raw_file = ti.xcom_pull(task_ids="fetch_live_data")
        
        # 1. Setup Paths
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        processed_dir_path = os.path.join(base_dir, "include", "data", "processed")
        sys.path.insert(0, os.path.join(base_dir, "src"))
        
        from preprocess import preprocess

        # 2. Run Logic
        processed_file = preprocess()

        # 3. DVC & Git Automation   
        # Check if folder has content
        if os.path.exists(processed_dir_path) and os.listdir(processed_dir_path):
            print("ğŸ¤– Starting Auto-Version Control...")
            
            try:
                # A. Add to DVC (using --no-scm to prevent it looking for local git)
                subprocess.run(["dvc", "add", processed_dir_path], check=True)

                # B. Configure Git User (Required inside Docker)
                subprocess.run(["git", "config", "user.email", "airflow-bot@example.com"], check=False)
                subprocess.run(["git", "config", "user.name", "Airflow Bot"], check=False)

                # C. Git Add (The .dvc file + .gitignore generated by DVC)
                # We use absolute paths to be safe
                dvc_file = f"{processed_dir_path}.dvc"
                gitignore_file = os.path.join(os.path.dirname(processed_dir_path), ".gitignore")
                
                subprocess.run(["git", "add", dvc_file, gitignore_file], check=True)
                
                # D. Git Commit
                subprocess.run(
                    ["git", "commit", "-m", f"Auto-commit: Processed data update {datetime.now()}"], 
                    check=True
                )
                print("âœ… Git Commit Successful!")
            
            except subprocess.CalledProcessError as e:
                print(f"âš  Git/DVC Command failed or nothing to commit: {e}")
                # We don't raise error here to let the DAG continue to training
                # even if git commit was empty (no changes)

        return processed_file

    preprocess_task_op = PythonOperator(
        task_id="preprocess",
        python_callable=preprocess_task,
    )

    # --------------------
    # Task 4: Train
    # --------------------
    def train_task(ti):
        processed_file = ti.xcom_pull(task_ids="preprocess")
        
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        sys.path.insert(0, os.path.join(base_dir, "src"))
        
        from train_model import train

        train(processed_file)

    train_task_op = PythonOperator(
        task_id="train_model",
        python_callable=train_task,
    )

    # --------------------
    # DAG Dependencies
    # --------------------
    fetch_task >> quality_task >> preprocess_task_op >> train_task_op